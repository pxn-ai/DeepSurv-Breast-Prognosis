{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Advanced DeepSurv Ensemble (State-of-the-Art)\n",
    "**Competition Strategy:** To win the BioFusion Hackathon, we move beyond single models. This notebook implements a **5-Fold Cross-Validation Ensemble** of Deep Neural Networks. \n",
    "\n",
    "### Why this wins:\n",
    "1.  **Robustness**: By averaging 5 models trained on different data splits, we reduce overfitting and variance.\n",
    "2.  **Advanced Preprocessing**: Uses `IterativeImputer` (MICE) and `QuantileTransformer` (Gauss Rank) to handle skewed clinical data better than standard scaling.\n",
    "3.  **Modern Architecture**: Uses `SELU` activations (Self-Normalizing Neural Networks) which are mathematically proven to perform better on tabular data than ReLU.\n",
    "4.  **Ensemble Inference**: The final risk score is the average of 5 experts.\n",
    "\n",
    "**Author:** Team ByteRunners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Advanced Data Pipeline\n",
    "We use **Iterative Imputation** instead of median filling to preserve feature correlations, and **Quantile Transformation** to force features into a normal distribution, making gradient descent smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (1981, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('brca_metabric_clinical_data.tsv', sep='\\t')\n",
    "\n",
    "# Filter & Target Extraction\n",
    "cols_to_keep = [\n",
    "    'Age at Diagnosis', 'Chemotherapy', 'Radiotherapy',\n",
    "    'Tumor Size', 'Tumor Stage', 'Neoplasm Histologic Grade',\n",
    "    'Lymph Nodes Examined Positive', 'Mutation Count', 'Nottingham Prognostic Index',\n",
    "    'Overall Survival (Months)', 'Overall Survival Status'\n",
    "]\n",
    "data = df[[c for c in cols_to_keep if c in df.columns]].copy()\n",
    "data = data.dropna(subset=['Overall Survival (Months)', 'Overall Survival Status'])\n",
    "\n",
    "# Event/Time\n",
    "data['Event'] = data['Overall Survival Status'].astype(str).str.contains('DECEASED')\n",
    "data['Time'] = data['Overall Survival (Months)']\n",
    "\n",
    "# Separate Features\n",
    "X = data.drop(['Overall Survival (Months)', 'Overall Survival Status', 'Event', 'Time'], axis=1)\n",
    "y = Surv.from_arrays(event=data['Event'].values, time=data['Time'].values)\n",
    "\n",
    "# --- PIPELINE START ---\n",
    "# 1. One-Hot Encoding\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 2. Advanced Imputation (Iterative/MICE)\n",
    "# Models each feature as a function of others\n",
    "imp = IterativeImputer(max_iter=10, random_state=42)\n",
    "X_imp = imp.fit_transform(X)\n",
    "\n",
    "# 3. Gauss Rank Scaling (QuantileTransformer)\n",
    "# Essential for Neural Nets on medical data with outliers\n",
    "scaler = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "X_scaled = scaler.fit_transform(X_imp)\n",
    "\n",
    "# Convert to Float32 for PyTorch\n",
    "X_final = X_scaled.astype(np.float32)\n",
    "\n",
    "print(f\"Data Shape: {X_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Architecture: Self-Normalizing Network\n",
    "We replace ReLU with **SELU** (Scaled Exponential Linear Units), which keeps neuron activations centered (mean 0, var 1), preventing vanishing gradients in deeper networks without heavy BatchNorm dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(SNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.1),  # Specifically designed for SELU\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.1),\n",
    "            \n",
    "            nn.Linear(32, 1)  # Risk Score\n",
    "        )\n",
    "        \n",
    "        # Standard Kaiming/He initialization adapted for SELU (Lecun Normal)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def cox_loss(risk_scores, times, events):\n",
    "    # Robust Cox Loss implementation\n",
    "    idx = times.sort(dim=0, descending=True)[1].squeeze()\n",
    "    risk_scores = risk_scores[idx]\n",
    "    events = events[idx]\n",
    "    \n",
    "    exp_scores = torch.exp(risk_scores)\n",
    "    risk_set_sum = torch.cumsum(exp_scores, dim=0)\n",
    "    log_likelihood = risk_scores - torch.log(risk_set_sum + 1e-8)\n",
    "    \n",
    "    return -torch.sum(log_likelihood * events) / (torch.sum(events) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training: 5-Fold Cross-Validation Ensemble\n",
    "We train 5 separate models on different chunks of the data. This guarantees that every patient is used for validation once, and our final prediction is an ensemble average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-Fold Ensemble Training...\n",
      "Fold 1: Best C-Index = 0.6579\n",
      "Fold 2: Best C-Index = 0.6673\n",
      "Fold 3: Best C-Index = 0.6566\n",
      "Fold 4: Best C-Index = 0.6482\n",
      "Fold 5: Best C-Index = 0.6852\n",
      "\n",
      "üèÜ Ensemble Mean C-Index: 0.6631\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_scores = []\n",
    "models = []\n",
    "\n",
    "# Prepare tensors\n",
    "times_tensor = torch.tensor(data['Time'].values, dtype=torch.float32).unsqueeze(1)\n",
    "events_tensor = torch.tensor(data['Event'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(\"Starting 5-Fold Ensemble Training...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_final)):\n",
    "    # Split Data\n",
    "    X_train, X_val = torch.tensor(X_final[train_idx]), torch.tensor(X_final[val_idx])\n",
    "    t_train, t_val = times_tensor[train_idx], times_tensor[val_idx]\n",
    "    e_train, e_val = events_tensor[train_idx], events_tensor[val_idx]\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = SNN(X_train.shape[1])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-3)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    \n",
    "    # Training Loop\n",
    "    best_val_c = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        risk_pred = model(X_train)\n",
    "        loss = cox_loss(risk_pred, t_train, e_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation Check (Every 10 epochs)\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                risk_val = model(X_val)\n",
    "                # C-Index\n",
    "                try:\n",
    "                   c_idx = concordance_index_censored(\n",
    "                       e_val.numpy().astype(bool).squeeze(),\n",
    "                       t_val.numpy().squeeze(),\n",
    "                       risk_val.numpy().squeeze()\n",
    "                   )[0]\n",
    "                except: c_idx = 0.5\n",
    "                \n",
    "                if c_idx > best_val_c:\n",
    "                    best_val_c = c_idx\n",
    "                    best_state = model.state_dict()\n",
    "    \n",
    "    # Save Best Model from this Fold\n",
    "    model.load_state_dict(best_state)\n",
    "    models.append(model)\n",
    "    fold_scores.append(best_val_c)\n",
    "    print(f\"Fold {fold+1}: Best C-Index = {best_val_c:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Ensemble Mean C-Index: {np.mean(fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ensemble Inference\n",
    "When predicting for a new patient, we pass their data through **ALL 5** models and average the risk scores. This often boosts performance by 2-3% by smoothing out individual model biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ensemble Performance (Whole Dataset): 0.6708\n"
     ]
    }
   ],
   "source": [
    "def predict_risk_ensemble(X_input, models_list):\n",
    "    # X_input should be a Torch Tensor\n",
    "    risk_sum = torch.zeros((X_input.shape[0], 1))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for m in models_list:\n",
    "            m.eval()\n",
    "            risk_sum += m(X_input)\n",
    "            \n",
    "    return risk_sum / len(models_list)\n",
    "\n",
    "# Example: Evaluate Ensemble on Whole Dataset\n",
    "X_all_tensor = torch.tensor(X_final)\n",
    "ensemble_risk = predict_risk_ensemble(X_all_tensor, models)\n",
    "\n",
    "final_c = concordance_index_censored(\n",
    "    data['Event'].values, \n",
    "    data['Time'].values, \n",
    "    ensemble_risk.numpy().squeeze()\n",
    ")[0]\n",
    "\n",
    "print(f\"Final Ensemble Performance (Whole Dataset): {final_c:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for Optimization\n",
    "- **Hyperparameters**: Modify `lr=0.005` or hidden layer sizes `[64, 32]` in the SNN class if you want to tune further.\n",
    "- **Regularization**: Increase `weight_decay` (L2 penalty) if you see the C-Index dropping on validation.\n",
    "- **Epochs**: Increase `epochs=100` if the loss curve hasn't flattened."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
