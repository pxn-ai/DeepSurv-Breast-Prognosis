{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f4e6d0",
   "metadata": {},
   "source": [
    "# DeepSurv-Breast-Prognosis\n",
    "**Team:** ByteRunners\n",
    "\n",
    "**Competition:** BioFusion Hackathon 2026\n",
    "\n",
    "**Problem:** Predicting 5-Year Survival Risk in Breast Cancer Patients.\n",
    "\n",
    "**Dataset:** METABRIC (Nature 2012), obtained from cBioPortal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b91074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Also suppress specific sklearn/torch warnings\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed precision training\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# GPU CONFIGURATION - RTX 3050 Optimization\n",
    "# ---------------------------------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üöÄ GPU Detected: {gpu_name}\")\n",
    "    print(f\"   VRAM: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Optimize for RTX 3050 (Ampere architecture)\n",
    "    torch.backends.cudnn.benchmark = True  # Auto-tune for best performance\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for faster matmul\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Running on CPU.\")\n",
    "\n",
    "print(f\"üìç Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 2: DATA LOADING & PREPROCESSING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# Load pre-filtered dataset (already contains selected clinical + gene expression features)\n",
    "df = pd.read_csv('data/choosen_data.csv')\n",
    "\n",
    "# Data Cleaning & Structuring\n",
    "# 1. Drop rows with missing targets.\n",
    "# 2. Convert Status to Boolean (Event = True/Dead, Censored = False/Living).\n",
    "data = df.dropna(subset=['Overall Survival (Months)', 'Overall Survival Status']).copy()\n",
    "\n",
    "# Remove the patients deceased by other causes (not breast cancer)\n",
    "data = data[~data[\"Patient's Vital Status\"].str.contains('OTHER', case=False, na=False)]\n",
    "\n",
    "data['Event'] = data['Overall Survival Status'].astype(str).apply(lambda x: True if 'DECEASED' in x.upper() else False)\n",
    "data['Time'] = data['Overall Survival (Months)']\n",
    "\n",
    "# Preprocessing Features\n",
    "# Drop non-feature columns: PATIENT_ID, target columns, and derived columns\n",
    "cols_to_drop = ['PATIENT_ID', 'Overall Survival (Months)', 'Overall Survival Status', \"Patient's Vital Status\", 'Event', 'Time']\n",
    "X = data.drop([c for c in cols_to_drop if c in data.columns], axis=1)\n",
    "\n",
    "# Impute missing numerical values with median\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "if len(num_cols) > 0:\n",
    "    imp_num = SimpleImputer(strategy='median')\n",
    "    X[num_cols] = imp_num.fit_transform(X[num_cols])\n",
    "\n",
    "# One-Hot Encode categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Constructing the Target Array\n",
    "# scikit-survival requires a structured array of (Event, Time) tuples.\n",
    "y = Surv.from_arrays(event=data['Event'].values, time=data['Time'].values)\n",
    "\n",
    "print(f\"Processed Data: {X.shape[0]} samples, {X.shape[1]} features.\")\n",
    "print(f\"Features: {list(X.columns[:10])}... (showing first 10)\")\n",
    "print(\"Target format example:\", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316fa50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 3: EXPLORATORY DATA ANALYSIS (Correlation Matrix)\n",
    "# ---------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Generating Clinical Correlation Matrix...\")\n",
    "\n",
    "# 1. Define the list of features you want to visualize\n",
    "# (This matches your raw CSV columns)\n",
    "viz_features = [\n",
    "    'Age at Diagnosis', 'Chemotherapy', 'Radiotherapy', 'Hormone Therapy',\n",
    "    'Tumor Size', 'Tumor Stage', 'Neoplasm Histologic Grade',\n",
    "    'Lymph Nodes Examined Positive', 'Mutation Count', 'Nottingham Prognostic Index',\n",
    "    'ER Status', 'HER2 Status', 'PR Status'\n",
    "]\n",
    "\n",
    "# 2. Create a temporary view of the data (Before One-Hot Encoding)\n",
    "# We use 'data' (the cleaned dataframe), NOT 'X' (the processed one)\n",
    "viz_df = data[viz_features].copy()\n",
    "\n",
    "# 3. THE MAGIC STEP: Convert Text to Numbers Automatically\n",
    "# This turns \"Positive\" -> 1, \"Negative\" -> 0, etc. without you doing anything manually.\n",
    "for col in viz_df.select_dtypes(include='object').columns:\n",
    "    viz_df[col] = viz_df[col].astype('category').cat.codes\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr = viz_df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix (Clinical Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 4: ADVANCED BIO-CLINICAL FEATURE ENGINEERING\n",
    "# ---------------------------------------------------------\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 1. Define Clinical Numeric Features\n",
    "# These are the standard \"hard numbers\" doctors use.\n",
    "clinical_numeric = [\n",
    "    'Age at Diagnosis',\n",
    "    'Tumor Size',\n",
    "    'Tumor Stage',\n",
    "    'Neoplasm Histologic Grade',\n",
    "    'Lymph Nodes Examined Positive',\n",
    "    'Mutation Count',\n",
    "    'Nottingham Prognostic Index'\n",
    "]\n",
    "\n",
    "# 2. Define Gene Expression Features (The new \"Bio\" data)\n",
    "# We select them automatically since they are all lowercase in your CSV\n",
    "# (e.g., 'esr1', 'brca1', 'tp53')\n",
    "gene_features = [col for col in X.columns if col.islower() and col not in ['time', 'event']]\n",
    "print(f\"Found {len(gene_features)} Gene Features for Interaction (e.g., {gene_features[:3]}...)\")\n",
    "\n",
    "# 3. Combine All Numeric Features\n",
    "numeric_feats = clinical_numeric + gene_features\n",
    "\n",
    "# 4. Find Binary Receptor Status Features (ER, PR, HER2)\n",
    "# These act as \"Switches\" for the interactions.\n",
    "# (e.g. Hormone Therapy works differently if ER Status is Positive)\n",
    "binary_genetic_feats = [col for col in X.columns if 'Status' in col and 'Positive' in col]\n",
    "\n",
    "# Also add Treatment flags if they exist (Chemo/Radio/Hormone)\n",
    "# They usually have '_YES' or '_1' suffix after One-Hot Encoding\n",
    "treatment_feats = [col for col in X.columns if 'Therapy' in col and ('YES' in col or '1' in col)]\n",
    "\n",
    "# 5. Master List for Polynomials\n",
    "interaction_input_cols = numeric_feats + binary_genetic_feats + treatment_feats\n",
    "\n",
    "# Filter to ensure they actally exist in X\n",
    "selected_cols = [c for c in interaction_input_cols if c in X.columns]\n",
    "\n",
    "print(f\"Generating Polynomials for {len(selected_cols)} features...\")\n",
    "\n",
    "# 6. Generate Interactions (Degree 2)\n",
    "# This creates powerful features like: \"esr1_expression * ER_Status_Positive\"\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly_bio = poly.fit_transform(X[selected_cols])\n",
    "\n",
    "# 7. Create DataFrame & Merge\n",
    "poly_cols = poly.get_feature_names_out(selected_cols)\n",
    "X_poly_df = pd.DataFrame(X_poly_bio, columns=poly_cols, index=X.index)\n",
    "\n",
    "# Merge: Original X + New Interactions\n",
    "X_final = pd.concat([X, X_poly_df], axis=1)\n",
    "X_final = X_final.loc[:, ~X_final.columns.duplicated()]\n",
    "\n",
    "print(f\"Original Features: {X.shape[1]}\")\n",
    "print(f\"Final Feature Count: {X_final.shape[1]}\")\n",
    "\n",
    "# 8. Update Tensors\n",
    "X_tensor = torch.tensor(X_final.values.astype(np.float32), dtype=torch.float32)\n",
    "X_tensor = (X_tensor - X_tensor.mean(dim=0)) / (X_tensor.std(dim=0) + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdeafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 5: PREPARE DATA FOR PYTORCH (GPU-Optimized)\n",
    "# ---------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert Data to PyTorch Tensors and move to GPU\n",
    "X_train_tensor = torch.tensor(X_train.values.astype(np.float32), dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.values.astype(np.float32), dtype=torch.float32).to(device)\n",
    "\n",
    "# Unpacking the Target\n",
    "def get_target_tensor(y_struct):\n",
    "    events = torch.tensor([x[0] for x in y_struct], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    times = torch.tensor([x[1] for x in y_struct], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    return times, events\n",
    "\n",
    "train_times, train_events = get_target_tensor(y_train)\n",
    "test_times, test_events = get_target_tensor(y_test)\n",
    "\n",
    "print(f\"‚úÖ Tensors loaded on: {device}\")\n",
    "print(f\"   Training samples: {X_train_tensor.shape[0]}, Features: {X_train_tensor.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 6: COX LOSS FUNCTION (GPU-Optimized)\n",
    "# ---------------------------------------------------------\n",
    "def cox_loss(risk_scores, times, events):\n",
    "    \"\"\"\n",
    "    Cox Partial Likelihood Loss - GPU Accelerated\n",
    "    Optimized for CUDA with efficient memory access patterns\n",
    "    \"\"\"\n",
    "    # 1. Sort by time (descending) - stays on GPU\n",
    "    idx = times.sort(dim=0, descending=True)[1].squeeze()\n",
    "    risk_scores = risk_scores[idx]\n",
    "    events = events[idx]\n",
    "\n",
    "    # 2. Compute Log-Sum-Exp of risk scores (GPU-accelerated cumsum)\n",
    "    exp_scores = torch.exp(risk_scores)\n",
    "    risk_set_sum = torch.cumsum(exp_scores, dim=0) \n",
    "    \n",
    "    # 3. Calculate Log-Likelihood\n",
    "    log_likelihood = risk_scores - torch.log(risk_set_sum + 1e-5)\n",
    "    \n",
    "    # 4. Negate because we want to Minimize Loss\n",
    "    loss = -torch.mean(log_likelihood * events)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 7: DeepSurv MODEL SELECTION (GPU-Accelerated Grid Search)\n",
    "# ---------------------------------------------------------\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 1. Define Candidate Architectures (can use larger networks with GPU)\n",
    "configs = [\n",
    "    {\"name\": \"Shallow\", \"layers\": [32, 16], \"dropout\": 0.1, \"lr\": 0.005},  \n",
    "    {\"name\": \"Deep\",    \"layers\": [128, 64, 32], \"dropout\": 0.3, \"lr\": 0.001}, \n",
    "    {\"name\": \"Robust\",  \"layers\": [64, 64], \"dropout\": 0.5, \"lr\": 0.001},\n",
    "    {\"name\": \"Wide\",    \"layers\": [256, 128], \"dropout\": 0.2, \"lr\": 0.001},  # New: GPU can handle this\n",
    "]\n",
    "\n",
    "# 2. Dynamic Model Builder (GPU-Compatible)\n",
    "class DynamicDeepSurv(nn.Module):\n",
    "    def __init__(self, input_dim, layers, dropout):\n",
    "        super(DynamicDeepSurv, self).__init__()\n",
    "        layer_list = []\n",
    "        in_nodes = input_dim\n",
    "        for out_nodes in layers:\n",
    "            layer_list.append(nn.Linear(in_nodes, out_nodes))\n",
    "            layer_list.append(nn.SELU())\n",
    "            layer_list.append(nn.BatchNorm1d(out_nodes))\n",
    "            layer_list.append(nn.Dropout(dropout))\n",
    "            in_nodes = out_nodes\n",
    "        layer_list.append(nn.Linear(in_nodes, 1))\n",
    "        self.network = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 3. K-Fold Cross-Validation Loop (GPU-Accelerated)\n",
    "def run_experiment(X, y_times, y_events, configs, k=5):\n",
    "    # Keep data on GPU for faster training\n",
    "    X = X.to(device)\n",
    "    y_times = y_times.to(device)\n",
    "    y_events = y_events.to(device)\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    \n",
    "    # Mixed precision scaler for RTX 3050\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    print(f\"üöÄ Starting {k}-Fold CV on {len(configs)} Architectures (GPU: {device})...\")\n",
    "    \n",
    "    for config in configs:\n",
    "        fold_scores = []\n",
    "        print(f\"\\nüîß Testing Architecture: {config['name']}\")\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X.cpu().numpy())):\n",
    "            # Convert indices to tensors for GPU indexing\n",
    "            train_idx_t = torch.tensor(train_idx, dtype=torch.long, device=device)\n",
    "            val_idx_t = torch.tensor(val_idx, dtype=torch.long, device=device)\n",
    "            \n",
    "            X_tr, X_val = X[train_idx_t], X[val_idx_t]\n",
    "            t_tr, t_val = y_times[train_idx_t], y_times[val_idx_t]\n",
    "            e_tr, e_val = y_events[train_idx_t], y_events[val_idx_t]\n",
    "            \n",
    "            # Initialize Model on GPU\n",
    "            model = DynamicDeepSurv(X.shape[1], config['layers'], config['dropout']).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "            \n",
    "            # Training with Mixed Precision (FP16)\n",
    "            for epoch in range(100):  # More epochs since GPU is faster\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with autocast():  # Automatic mixed precision\n",
    "                    risk = model(X_tr)\n",
    "                    loss = cox_loss(risk, t_tr, e_tr)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_risk = model(X_val)\n",
    "                \n",
    "            try:\n",
    "                c_index = concordance_index_censored(\n",
    "                    e_val.squeeze().bool().cpu().numpy(), \n",
    "                    t_val.squeeze().cpu().numpy(), \n",
    "                    val_risk.squeeze().cpu().numpy()\n",
    "                )[0]\n",
    "                fold_scores.append(c_index)\n",
    "            except:\n",
    "                pass \n",
    "        \n",
    "        avg_score = np.mean(fold_scores)\n",
    "        print(f\"   ‚úÖ Avg C-Index: {avg_score:.4f}\")\n",
    "        results.append((config['name'], avg_score))\n",
    "    \n",
    "    # Clear GPU cache after grid search\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "experiment_results = run_experiment(X_train_tensor, train_times, train_events, configs)\n",
    "\n",
    "# Display best architecture\n",
    "best_arch = max(experiment_results, key=lambda x: x[1])\n",
    "print(f\"\\nüèÜ Best Architecture: {best_arch[0]} (C-Index: {best_arch[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 8: TRAIN THE WINNING DeepSurv MODEL (GPU-Accelerated)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Initialize the Best Model on GPU\n",
    "best_model = DynamicDeepSurv(input_dim=X_train_tensor.shape[1], layers=[32, 16], dropout=0.1).to(device)\n",
    "\n",
    "# 2. Define Optimizer\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "\n",
    "# 3. Mixed Precision Training Setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(f\"üèÜ Training Final Champion Neural Network on {device}...\")\n",
    "print(f\"   Architecture: Shallow [32, 16]\")\n",
    "\n",
    "# 4. Train with GPU Acceleration + Mixed Precision\n",
    "epoch_losses = []\n",
    "for epoch in range(300):  # More epochs since GPU is fast\n",
    "    best_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast():  # FP16 for faster computation on RTX 3050\n",
    "        risk = best_model(X_train_tensor)\n",
    "        loss = cox_loss(risk, train_times, train_events)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    epoch_losses.append(loss.item())\n",
    "    \n",
    "    # Progress indicator every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"   Epoch {epoch+1}/300 - Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"‚úÖ 'best_model' is now trained and ready for the Ensemble step.\")\n",
    "\n",
    "# Plot the training curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epoch_losses, color='#e74c3c', linewidth=1.5)\n",
    "plt.title(\"DeepSurv Training Loss (GPU-Accelerated)\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cox Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüìä GPU Memory Used: {torch.cuda.memory_allocated()/1e6:.1f} MB\")\n",
    "    print(f\"   GPU Memory Cached: {torch.cuda.memory_reserved()/1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703aeb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 9: RANDOM SURVIVAL FOREST WITH GRID SEARCH\n",
    "# ---------------------------------------------------------\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# 1. Use the Polynomial Features (X_final from previous steps)\n",
    "# We need to re-split because X_final has the interaction terms\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training RSF on {X_train_poly.shape[1]} features (including interactions)...\")\n",
    "\n",
    "# 2. Define Hyperparameter Grid (The \"Model Selection\" Step)\n",
    "param_grid = {\n",
    "    'n_estimators': [500],\n",
    "    'min_samples_split': [10, 15, 20],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "# 3. Run Grid Search (5-Fold CV)\n",
    "rsf = RandomSurvivalForest(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rsf,\n",
    "    param_grid=param_grid,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Grid Search for Random Forest (this may take 2-3 mins)...\")\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "\n",
    "# 4. Get Best Model\n",
    "best_rsf = grid_search.best_estimator_\n",
    "print(f\"\\n‚úÖ Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"‚úÖ Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 5. Final Evaluation on Test Set\n",
    "rsf_score = best_rsf.score(X_test_poly, y_test)\n",
    "print(f\"\\nüéØ Optimized RSF C-Index: {rsf_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 10: ENSEMBLE MODEL (DeepSurv + RSF) - GPU Optimized\n",
    "# ---------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from scipy.stats import zscore\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "print(f\"üîß Building Ensemble Model (GPU: {device})...\")\n",
    "\n",
    "# 1. Re-Create Train/Test Split using the POLYNOMIAL Data (X_final)\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Convert to PyTorch Tensors on GPU\n",
    "X_train_tensor_poly = torch.tensor(X_train_poly.values.astype(np.float32), dtype=torch.float32).to(device)\n",
    "X_test_tensor_poly = torch.tensor(X_test_poly.values.astype(np.float32), dtype=torch.float32).to(device)\n",
    "\n",
    "# Normalize on GPU (faster than CPU)\n",
    "mean = X_train_tensor_poly.mean(dim=0)\n",
    "std = X_train_tensor_poly.std(dim=0) + 1e-5\n",
    "X_train_tensor_poly = (X_train_tensor_poly - mean) / std\n",
    "X_test_tensor_poly = (X_test_tensor_poly - mean) / std\n",
    "\n",
    "# 3. Re-Initialize Model on GPU\n",
    "input_dim = X_train_tensor_poly.shape[1]\n",
    "print(f\"   Model Input Dimension: {input_dim}\")\n",
    "\n",
    "best_model = DynamicDeepSurv(input_dim=input_dim, layers=[32, 16], dropout=0.1).to(device)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 4. Re-Train on the Polynomial Data (GPU + Mixed Precision)\n",
    "print(\"   Re-Training Neural Network on Polynomial Features...\")\n",
    "for epoch in range(300): \n",
    "    best_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast():\n",
    "        risk = best_model(X_train_tensor_poly)\n",
    "        loss = cox_loss(risk, train_times, train_events)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "# 5. Generate Final Predictions\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    with autocast():  # Use FP16 for inference too\n",
    "        risk_deep = best_model(X_test_tensor_poly).cpu().numpy().flatten()\n",
    "\n",
    "# 6. Get RSF Predictions (CPU-based, but fast)\n",
    "risk_rf = best_rsf.predict(X_test_poly)\n",
    "\n",
    "# 7. ENSEMBLE: Normalize and Average\n",
    "risk_deep_norm = zscore(risk_deep)\n",
    "risk_rf_norm = zscore(risk_rf)\n",
    "\n",
    "# Weighted Average (0.6 RF + 0.4 Deep)\n",
    "ensemble_risk = (0.6 * risk_rf_norm) + (0.4 * risk_deep_norm)\n",
    "\n",
    "# 8. Calculate Final Scores\n",
    "test_events_cpu = test_events.squeeze().bool().cpu().numpy()\n",
    "test_times_cpu = test_times.squeeze().cpu().numpy()\n",
    "\n",
    "c_deep = concordance_index_censored(test_events_cpu, test_times_cpu, risk_deep)[0]\n",
    "c_rf = concordance_index_censored(test_events_cpu, test_times_cpu, risk_rf)[0]\n",
    "c_ens = concordance_index_censored(test_events_cpu, test_times_cpu, ensemble_risk)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üèÜ FINAL RESULTS TABLE\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Random Forest:     {c_rf:.4f}\")\n",
    "print(f\"DeepSurv (GPU):    {c_deep:.4f}\")\n",
    "print(f\"ENSEMBLE:          {c_ens:.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n‚úÖ GPU Memory Cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4321149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 11: PREDICTION VISUALIZATION & MODEL COMPARISON\n",
    "# ---------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# ============================\n",
    "# 1. Head View of Final Predictions\n",
    "# ============================\n",
    "\n",
    "# Extract Event and Time from structured array y_test\n",
    "event_status = np.array([x[0] for x in y_test])  # Event (True=Deceased, False=Alive)\n",
    "survival_months = np.array([x[1] for x in y_test])  # Time in months\n",
    "\n",
    "# Create a comprehensive results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'RSF_Risk': risk_rf,\n",
    "    'DeepSurv_Risk': risk_deep,\n",
    "    'Ensemble_Risk': ensemble_risk,\n",
    "    'RSF_Risk_Norm': risk_rf_norm,\n",
    "    'DeepSurv_Risk_Norm': risk_deep_norm,\n",
    "    'Survival_Months': survival_months,\n",
    "    'Event_Status': event_status,\n",
    "    'Status_Label': ['Deceased' if s else 'Alive' for s in event_status]\n",
    "})\n",
    "\n",
    "# Sort by ensemble risk (highest risk first)\n",
    "results_df_sorted = results_df.sort_values('Ensemble_Risk', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä TOP 15 HIGHEST RISK PATIENTS (Ensemble Prediction)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df_sorted[['RSF_Risk_Norm', 'DeepSurv_Risk_Norm', 'Ensemble_Risk', \n",
    "                          'Survival_Months', 'Status_Label']].head(15).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TOP 15 LOWEST RISK PATIENTS (Ensemble Prediction)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df_sorted[['RSF_Risk_Norm', 'DeepSurv_Risk_Norm', 'Ensemble_Risk', \n",
    "                          'Survival_Months', 'Status_Label']].tail(15).to_string())\n",
    "\n",
    "# ============================\n",
    "# 2. Model Comparison Visualization\n",
    "# ============================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('üî¨ Three-Model Prediction Comparison: RSF vs DeepSurv vs Ensemble', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# --- Plot 1: Overlapping Distribution of Risk Scores ---\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(risk_rf_norm, bins=40, alpha=0.5, label=f'RSF (C={c_rf:.4f})', color='blue', edgecolor='black')\n",
    "ax1.hist(risk_deep_norm, bins=40, alpha=0.5, label=f'DeepSurv (C={c_deep:.4f})', color='green', edgecolor='black')\n",
    "ax1.hist(ensemble_risk, bins=40, alpha=0.5, label=f'Ensemble (C={c_ens:.4f})', color='red', edgecolor='black')\n",
    "ax1.set_xlabel('Normalized Risk Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Risk Scores (All Models)')\n",
    "ax1.legend()\n",
    "\n",
    "# --- Plot 2: Scatter - RSF vs DeepSurv Predictions ---\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['red' if s else 'green' for s in event_status]\n",
    "ax2.scatter(risk_rf_norm, risk_deep_norm, c=colors, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "ax2.plot([-3, 3], [-3, 3], 'k--', alpha=0.5, label='Perfect Agreement')\n",
    "ax2.set_xlabel('RSF Risk Score (Normalized)')\n",
    "ax2.set_ylabel('DeepSurv Risk Score (Normalized)')\n",
    "ax2.set_title('RSF vs DeepSurv Agreement')\n",
    "legend_elements = [Patch(facecolor='red', label='Deceased'),\n",
    "                   Patch(facecolor='green', label='Alive')]\n",
    "ax2.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "# --- Plot 3: Bar Chart - C-Index Comparison ---\n",
    "ax3 = axes[0, 2]\n",
    "models = ['Random\\nSurvival Forest', 'DeepSurv\\n(Neural Net)', 'Ensemble\\n(60% RF + 40% Deep)']\n",
    "c_indices = [c_rf, c_deep, c_ens]\n",
    "colors_bar = ['#3498db', '#27ae60', '#e74c3c']\n",
    "bars = ax3.bar(models, c_indices, color=colors_bar, edgecolor='black', linewidth=1.5)\n",
    "ax3.set_ylabel('C-Index Score')\n",
    "ax3.set_title('Model Performance Comparison')\n",
    "ax3.set_ylim(0.5, max(c_indices) + 0.05)\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random Baseline')\n",
    "for bar, c_val in zip(bars, c_indices):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{c_val:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "ax3.legend()\n",
    "\n",
    "# --- Plot 4: Patient Ranking Comparison ---\n",
    "ax4 = axes[1, 0]\n",
    "patient_indices = np.arange(len(risk_rf_norm))\n",
    "sorted_idx_ens = np.argsort(ensemble_risk)\n",
    "ax4.scatter(patient_indices, risk_rf_norm[sorted_idx_ens], alpha=0.6, s=15, label='RSF', color='blue')\n",
    "ax4.scatter(patient_indices, risk_deep_norm[sorted_idx_ens], alpha=0.6, s=15, label='DeepSurv', color='green')\n",
    "ax4.scatter(patient_indices, ensemble_risk[sorted_idx_ens], alpha=0.8, s=15, label='Ensemble', color='red')\n",
    "ax4.set_xlabel('Patient Rank (by Ensemble Risk)')\n",
    "ax4.set_ylabel('Normalized Risk Score')\n",
    "ax4.set_title('Risk Predictions Across All Patients')\n",
    "ax4.legend()\n",
    "\n",
    "# --- Plot 5: Box Plot by Event Status ---\n",
    "ax5 = axes[1, 1]\n",
    "alive_mask = event_status == False\n",
    "deceased_mask = event_status == True\n",
    "\n",
    "box_data = [\n",
    "    risk_rf_norm[alive_mask], risk_rf_norm[deceased_mask],\n",
    "    risk_deep_norm[alive_mask], risk_deep_norm[deceased_mask],\n",
    "    ensemble_risk[alive_mask], ensemble_risk[deceased_mask]\n",
    "]\n",
    "box_labels = ['RSF\\nAlive', 'RSF\\nDeceased', 'Deep\\nAlive', 'Deep\\nDeceased', 'Ens\\nAlive', 'Ens\\nDeceased']\n",
    "bp = ax5.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "colors_box = ['lightgreen', 'lightcoral'] * 3\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "ax5.set_ylabel('Normalized Risk Score')\n",
    "ax5.set_title('Risk Distribution by Survival Status')\n",
    "ax5.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# --- Plot 6: Risk Group Analysis ---\n",
    "ax6 = axes[1, 2]\n",
    "risk_quantiles = pd.qcut(ensemble_risk, q=4, labels=['Low Risk', 'Medium-Low', 'Medium-High', 'High Risk'])\n",
    "risk_analysis = pd.DataFrame({\n",
    "    'Risk_Group': risk_quantiles,\n",
    "    'RSF_Risk': risk_rf_norm,\n",
    "    'DeepSurv_Risk': risk_deep_norm,\n",
    "    'Ensemble_Risk': ensemble_risk,\n",
    "    'Survival_Months': survival_months,\n",
    "    'Event': event_status\n",
    "})\n",
    "\n",
    "group_stats = risk_analysis.groupby('Risk_Group').agg({\n",
    "    'RSF_Risk': 'mean',\n",
    "    'DeepSurv_Risk': 'mean', \n",
    "    'Ensemble_Risk': 'mean',\n",
    "    'Event': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "x = np.arange(len(group_stats))\n",
    "width = 0.25\n",
    "ax6.bar(x - width, group_stats['RSF_Risk'], width, label='RSF', color='#3498db')\n",
    "ax6.bar(x, group_stats['DeepSurv_Risk'], width, label='DeepSurv', color='#27ae60')\n",
    "ax6.bar(x + width, group_stats['Ensemble_Risk'], width, label='Ensemble', color='#e74c3c')\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(group_stats.index, rotation=15)\n",
    "ax6.set_ylabel('Mean Risk Score')\n",
    "ax6.set_title('Mean Risk by Quartile Group')\n",
    "ax6.legend()\n",
    "\n",
    "for i, (idx, row) in enumerate(group_stats.iterrows()):\n",
    "    ax6.text(i, max(row['RSF_Risk'], row['DeepSurv_Risk'], row['Ensemble_Risk']) + 0.1, \n",
    "             f'Mort: {row[\"Event\"]*100:.0f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 3. Summary Statistics Table\n",
    "# ============================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<25} {'C-Index':<12} {'Mean Risk':<15} {'Std Risk':<15}\")\n",
    "print(\"-\"*67)\n",
    "print(f\"{'Random Survival Forest':<25} {c_rf:<12.4f} {risk_rf_norm.mean():<15.4f} {risk_rf_norm.std():<15.4f}\")\n",
    "print(f\"{'DeepSurv Neural Network':<25} {c_deep:<12.4f} {risk_deep_norm.mean():<15.4f} {risk_deep_norm.std():<15.4f}\")\n",
    "print(f\"{'Ensemble (60% RF + 40% DL)':<25} {c_ens:<12.4f} {ensemble_risk.mean():<15.4f} {ensemble_risk.std():<15.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Model comparison visualization saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62656221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 12: FEATURE IMPORTANCE (Permutation)\n",
    "# ---------------------------------------------------------\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚è≥ Calculating Feature Importance (this takes ~1-2 minutes)...\")\n",
    "\n",
    "# 1. Calculate Importance on Test Set\n",
    "result = permutation_importance(\n",
    "    best_rsf, X_test_poly, y_test, n_repeats=5, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2. Sort and Select Top 15 Features\n",
    "sorted_idx = result.importances_mean.argsort()[-15:]\n",
    "top_features = X_test_poly.columns[sorted_idx]\n",
    "top_scores = result.importances[sorted_idx]\n",
    "\n",
    "# 3. Enhanced Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "box = plt.boxplot(\n",
    "    top_scores.T,\n",
    "    vert=False,\n",
    "    labels=top_features,\n",
    "    patch_artist=True,\n",
    "    widths=0.7\n",
    ")\n",
    "\n",
    "# Apply Professional Coloring\n",
    "for patch in box['boxes']:\n",
    "    patch.set_facecolor('#2ecc71')\n",
    "    patch.set_alpha(0.6)\n",
    "    patch.set_edgecolor('black')\n",
    "\n",
    "for median in box['medians']:\n",
    "    median.set_color('black')\n",
    "    median.set_linewidth(1.5)\n",
    "\n",
    "# Add Guidelines\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "plt.axvline(x=0, color='red', linestyle='-', linewidth=1, label=\"No Impact\")\n",
    "\n",
    "# Labels\n",
    "plt.title(\"Top 15 Drivers of Survival Prediction (Feature Importance)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Decrease in Model Accuracy (C-Index) when Feature is Removed\", fontsize=12)\n",
    "plt.ylabel(\"Clinical Features & Interactions\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Print Summary for Report\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üèÜ TOP 5 PREDICTIVE FEATURES\")\n",
    "print(\"=\"*40)\n",
    "for i in range(1, 6):\n",
    "    idx = sorted_idx[-i]\n",
    "    print(f\"{i}. {X_test_poly.columns[idx]} (Impact: {result.importances_mean[idx]:.4f})\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 13: SHAP ANALYSIS (GPU-Accelerated Model Explainability)\n",
    "# ---------------------------------------------------------\n",
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"üöÄ Initializing SHAP Analysis (GPU: {device})...\")\n",
    "\n",
    "# 1. Define the Ensemble Wrapper Function (GPU-Optimized)\n",
    "def ensemble_predict_wrapper(X_numpy):\n",
    "    if isinstance(X_numpy, np.ndarray):\n",
    "        X_df = pd.DataFrame(X_numpy, columns=X_test_poly.columns)\n",
    "    else:\n",
    "        X_df = X_numpy\n",
    "\n",
    "    # Random Forest Prediction (CPU)\n",
    "    risk_rf = best_rsf.predict(X_df)\n",
    "\n",
    "    # DeepSurv Prediction (GPU-Accelerated)\n",
    "    X_tensor = torch.tensor(X_df.values.astype(np.float32), dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Use pre-computed normalization stats (already on GPU)\n",
    "    X_tensor_norm = (X_tensor - mean) / std\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast():  # FP16 inference\n",
    "            risk_deep = best_model(X_tensor_norm).cpu().numpy().flatten()\n",
    "\n",
    "    return (0.6 * risk_rf) + (0.4 * risk_deep)\n",
    "\n",
    "# 2. Prepare Background Data (Summary)\n",
    "print(\"‚öôÔ∏è  Summarizing Training Data for Baseline...\")\n",
    "X_train_summary = shap.kmeans(X_train_poly, 20)\n",
    "\n",
    "# 3. Initialize Kernel Explainer on the ENSEMBLE\n",
    "explainer = shap.KernelExplainer(ensemble_predict_wrapper, X_train_summary)\n",
    "\n",
    "# 4. Run on the Full Test Set\n",
    "print(f\"‚ö° Running SHAP on all {X_test_poly.shape[0]} test patients...\")\n",
    "print(\"   (GPU acceleration active for DeepSurv predictions)\")\n",
    "shap_values = explainer.shap_values(X_test_poly)\n",
    "\n",
    "# 5. Plot the Results\n",
    "print(\"üé® Generating Plots...\")\n",
    "\n",
    "# Summary Plot (Global Feature Importance)\n",
    "plt.figure()\n",
    "plt.title(\"What drives risk in the Hybrid Ensemble?\")\n",
    "shap.summary_plot(shap_values, X_test_poly, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top Interaction Plot\n",
    "top_feature = X_test_poly.columns[0]\n",
    "shap.dependence_plot(top_feature, shap_values, X_test_poly)\n",
    "\n",
    "# Clear GPU cache after SHAP analysis\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n‚úÖ SHAP Analysis Complete (GPU Memory Cleared)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63069f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 14: GPU PERFORMANCE SUMMARY\n",
    "# ---------------------------------------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"üìä GPU ACCELERATION SUMMARY - RTX 3050\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüñ•Ô∏è  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"   CUDA Cores: ~2560 (Ampere)\")\n",
    "    print(f\"   Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    print(f\"\\n‚ö° Optimizations Applied:\")\n",
    "    print(f\"   ‚úÖ CUDA Backend Enabled\")\n",
    "    print(f\"   ‚úÖ cuDNN Benchmark Mode (Auto-tuning)\")\n",
    "    print(f\"   ‚úÖ TF32 Precision (Ampere feature)\")\n",
    "    print(f\"   ‚úÖ Mixed Precision Training (FP16)\")\n",
    "    print(f\"   ‚úÖ Automatic Memory Management\")\n",
    "    \n",
    "    print(f\"\\nüìà Expected Speedup vs CPU:\")\n",
    "    print(f\"   Neural Network Training: ~5-10x faster\")\n",
    "    print(f\"   Model Inference: ~3-5x faster\")\n",
    "    print(f\"   Large Batch Processing: ~8-15x faster\")\n",
    "    \n",
    "    # Final memory stats\n",
    "    print(f\"\\nüíæ Current GPU Memory:\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated()/1e6:.1f} MB\")\n",
    "    print(f\"   Cached: {torch.cuda.memory_reserved()/1e6:.1f} MB\")\n",
    "    print(f\"   Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved())/1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available - running on CPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ Model Performance Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Random Survival Forest: {c_rf:.4f}\")\n",
    "print(f\"   DeepSurv (GPU):         {c_deep:.4f}\")\n",
    "print(f\"   Ensemble (Hybrid):      {c_ens:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
