{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f4e6d0",
   "metadata": {},
   "source": [
    "# DeepSurv-Breast-Prognosis\n",
    "**Team:** ByteRunners\n",
    "\n",
    "**Competition:** BioFusion Hackathon 2026\n",
    "\n",
    "**Problem:** Predicting 5-Year Survival Risk in Breast Cancer Patients.\n",
    "\n",
    "**Dataset:** METABRIC (Nature 2012), obtained from cBioPortal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b91074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Also suppress specific sklearn/torch warnings\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 2: DATA LOADING & PREPROCESSING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# Load pre-filtered dataset (already contains selected clinical + gene expression features)\n",
    "df = pd.read_csv('data/choosen_data.csv')\n",
    "\n",
    "# Data Cleaning & Structuring\n",
    "# 1. Drop rows with missing targets.\n",
    "# 2. Convert Status to Boolean (Event = True/Dead, Censored = False/Living).\n",
    "data = df.dropna(subset=['Overall Survival (Months)', 'Overall Survival Status']).copy()\n",
    "\n",
    "# Remove the patients deceased by other causes (not breast cancer)\n",
    "data = data[~data[\"Patient's Vital Status\"].str.contains('OTHER', case=False, na=False)]\n",
    "\n",
    "data['Event'] = data['Overall Survival Status'].astype(str).apply(lambda x: True if 'DECEASED' in x.upper() else False)\n",
    "data['Time'] = data['Overall Survival (Months)']\n",
    "\n",
    "# Preprocessing Features\n",
    "# Drop non-feature columns: PATIENT_ID, target columns, and derived columns\n",
    "cols_to_drop = ['PATIENT_ID', 'Overall Survival (Months)', 'Overall Survival Status', \"Patient's Vital Status\", 'Event', 'Time']\n",
    "X = data.drop([c for c in cols_to_drop if c in data.columns], axis=1)\n",
    "\n",
    "# Impute missing numerical values with median\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "if len(num_cols) > 0:\n",
    "    imp_num = SimpleImputer(strategy='median')\n",
    "    X[num_cols] = imp_num.fit_transform(X[num_cols])\n",
    "\n",
    "# One-Hot Encode categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Constructing the Target Array\n",
    "# scikit-survival requires a structured array of (Event, Time) tuples.\n",
    "y = Surv.from_arrays(event=data['Event'].values, time=data['Time'].values)\n",
    "\n",
    "print(f\"Processed Data: {X.shape[0]} samples, {X.shape[1]} features.\")\n",
    "print(f\"Features: {list(X.columns[:10])}... (showing first 10)\")\n",
    "print(\"Target format example:\", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316fa50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 3: EXPLORATORY DATA ANALYSIS (Correlation Matrix)\n",
    "# ---------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Generating Clinical Correlation Matrix...\")\n",
    "\n",
    "# 1. Define the list of features you want to visualize\n",
    "# (This matches your raw CSV columns)\n",
    "viz_features = [\n",
    "    'Age at Diagnosis', 'Chemotherapy', 'Radiotherapy', 'Hormone Therapy',\n",
    "    'Tumor Size', 'Tumor Stage', 'Neoplasm Histologic Grade',\n",
    "    'Lymph Nodes Examined Positive', 'Mutation Count', 'Nottingham Prognostic Index',\n",
    "    'ER Status', 'HER2 Status', 'PR Status'\n",
    "]\n",
    "\n",
    "# 2. Create a temporary view of the data (Before One-Hot Encoding)\n",
    "# We use 'data' (the cleaned dataframe), NOT 'X' (the processed one)\n",
    "viz_df = data[viz_features].copy()\n",
    "\n",
    "# 3. THE MAGIC STEP: Convert Text to Numbers Automatically\n",
    "# This turns \"Positive\" -> 1, \"Negative\" -> 0, etc. without you doing anything manually.\n",
    "for col in viz_df.select_dtypes(include='object').columns:\n",
    "    viz_df[col] = viz_df[col].astype('category').cat.codes\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr = viz_df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix (Clinical Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b71bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 4: FEATURE ENGINEERING (Polynomial Interactions)\n",
    "# ---------------------------------------------------------\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create Interaction Terms (Degree 2)\n",
    "# This creates: Age*Size, Age*Stage, etc.\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Create DataFrame for new features\n",
    "poly_cols = poly.get_feature_names_out(X.columns)\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly_cols, index=X.index)\n",
    "\n",
    "# Merge back with original data (Drop duplicates if needed)\n",
    "# We keep original features + new interactions\n",
    "X_final = pd.concat([X, X_poly_df], axis=1)\n",
    "\n",
    "# Remove duplicate columns if any (e.g. original features might be repeated)\n",
    "X_final = X_final.loc[:, ~X_final.columns.duplicated()]\n",
    "\n",
    "print(f\"New Feature Count: {X_final.shape[1]} (Added interactions like {poly_cols[-3:]})\")\n",
    "\n",
    "# UPDATE TENSORS\n",
    "X_tensor = torch.tensor(X_final.values.astype(np.float32), dtype=torch.float32)\n",
    "# Re-normalize since we added new features with large values (squares)\n",
    "X_tensor = (X_tensor - X_tensor.mean(dim=0)) / (X_tensor.std(dim=0) + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdeafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 5: PREPARE DATA FOR PYTORCH\n",
    "# ---------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert Data to PyTorch Tensors\n",
    "# PyTorch requires float32 tensors for the neural network.\n",
    "X_train_tensor = torch.tensor(X_train.values.astype(np.float32), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "# Unpacking the Target\n",
    "# DeepSurv requires separate Tensors for 'Time' and 'Event', not a structured array.\n",
    "def get_target_tensor(y_struct):\n",
    "    events = torch.tensor([x[0] for x in y_struct], dtype=torch.float32).unsqueeze(1)\n",
    "    times = torch.tensor([x[1] for x in y_struct], dtype=torch.float32).unsqueeze(1)\n",
    "    return times, events\n",
    "\n",
    "train_times, train_events = get_target_tensor(y_train)\n",
    "test_times, test_events = get_target_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 6: COX LOSS FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "def cox_loss(risk_scores, times, events):\n",
    "    # Cox Partial Likelihood Loss\n",
    "    # We want to maximize the probability that the patient who died had a higher risk score \n",
    "    # than all other patients who were alive at that time (the \"Risk Set\").\n",
    "    \n",
    "    # 1. Sort by time (descending) to easily compute the Risk Set\n",
    "    idx = times.sort(dim=0, descending=True)[1].squeeze()\n",
    "    risk_scores = risk_scores[idx]\n",
    "    events = events[idx]\n",
    "\n",
    "    # 2. Compute Log-Sum-Exp of risk scores (The \"Risk Set\" denominator)\n",
    "    # cumsum allows us to efficiently calculate the sum of risks for everyone 'at risk'\n",
    "    exp_scores = torch.exp(risk_scores)\n",
    "    risk_set_sum = torch.cumsum(exp_scores, dim=0) \n",
    "    \n",
    "    # 3. Calculate Log-Likelihood\n",
    "    # log(risk / sum(risk_at_risk)) = log(risk) - log(sum(risk_at_risk))\n",
    "    log_likelihood = risk_scores - torch.log(risk_set_sum + 1e-5) # 1e-5 for stability\n",
    "    \n",
    "    # 4. Negate because we want to Minimize Loss (Maximize Likelihood)\n",
    "    # Only events (deaths) contribute to the loss numerator.\n",
    "    loss = -torch.mean(log_likelihood * events)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 7: DeepSurv MODEL SELECTION (GRID SEARCH)\n",
    "# ---------------------------------------------------------\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 1. Define Candidate Architectures\n",
    "configs = [\n",
    "    {\"name\": \"Shallow\", \"layers\": [32, 16], \"dropout\": 0.1, \"lr\": 0.005},  \n",
    "    # Faster convergence\n",
    "    {\"name\": \"Deep\",    \"layers\": [128, 64, 32], \"dropout\": 0.3, \"lr\": 0.001}, \n",
    "    # Captures complexity\n",
    "    {\"name\": \"Robust\",  \"layers\": [64, 64], \"dropout\": 0.5, \"lr\": 0.001}     \n",
    "    # Prevents overfitting\n",
    "]\n",
    "\n",
    "# 2. Dynamic Model Builder\n",
    "class DynamicDeepSurv(nn.Module):\n",
    "    def __init__(self, input_dim, layers, dropout):\n",
    "        super(DynamicDeepSurv, self).__init__()\n",
    "        layer_list = []\n",
    "        in_nodes = input_dim\n",
    "        for out_nodes in layers:\n",
    "            layer_list.append(nn.Linear(in_nodes, out_nodes))\n",
    "            layer_list.append(nn.SELU()) # SELU is excellent for medical data\n",
    "            layer_list.append(nn.BatchNorm1d(out_nodes))\n",
    "            layer_list.append(nn.Dropout(dropout))\n",
    "            in_nodes = out_nodes\n",
    "        layer_list.append(nn.Linear(in_nodes, 1))\n",
    "        self.network = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 3. K-Fold Cross-Validation Loop\n",
    "def run_experiment(X, y_times, y_events, configs, k=5):\n",
    "    # Ensure inputs are CPU tensors for slicing\n",
    "    X = X.cpu()\n",
    "    y_times = y_times.cpu()\n",
    "    y_events = y_events.cpu()\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    print(f\"Starting {k}-Fold Cross-Validation on {len(configs)} Architectures...\")\n",
    "    \n",
    "    for config in configs:\n",
    "        fold_scores = []\n",
    "        print(f\"\\nTesting Architecture: {config['name']}\")\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            # Slice Tensors using the indices\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            t_tr, t_val = y_times[train_idx], y_times[val_idx]\n",
    "            e_tr, e_val = y_events[train_idx], y_events[val_idx]\n",
    "            \n",
    "            # Initialize & Train\n",
    "            model = DynamicDeepSurv(X.shape[1], config['layers'], config['dropout'])\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "            \n",
    "            for epoch in range(80): # 80 epochs is enough for selection\n",
    "                model.train()\n",
    "                risk = model(X_tr)\n",
    "                loss = cox_loss(risk, t_tr, e_tr)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_risk = model(X_val)\n",
    "                \n",
    "            try:\n",
    "                # Calculate C-Index on Validation Fold\n",
    "                c_index = concordance_index_censored(\n",
    "                    e_val.squeeze().bool().numpy(), \n",
    "                    t_val.squeeze().numpy(), \n",
    "                    val_risk.squeeze().numpy()\n",
    "                )[0]\n",
    "                fold_scores.append(c_index)\n",
    "            except:\n",
    "                pass \n",
    "        \n",
    "        avg_score = np.mean(fold_scores)\n",
    "        print(f\"--> Avg C-Index: {avg_score:.4f}\")\n",
    "        results.append((config['name'], avg_score))\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "experiment_results = run_experiment(X_train_tensor, train_times, train_events, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 8: TRAIN THE WINNING DeepSurv MODEL\n",
    "# ---------------------------------------------------------\n",
    "# We selected \"Shallow\" based on Grid Search result\n",
    "# Config: Layers=[32, 16], Dropout=0.1, LR=0.005\n",
    "\n",
    "# 1. Initialize the Best Model\n",
    "best_model = DynamicDeepSurv(input_dim=X_train_tensor.shape[1], layers=[32, 16], dropout=0.1)\n",
    "\n",
    "# 2. Define Optimizer (using the winning Learning Rate)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "\n",
    "print(\"üèÜ Training Final Champion Neural Network (Shallow Architecture)...\")\n",
    "\n",
    "# 3. Train on Full Data\n",
    "epoch_losses = []\n",
    "for epoch in range(200): # Training for 200 epochs for maximum convergence\n",
    "    best_model.train()\n",
    "    \n",
    "    # Forward Pass\n",
    "    risk = best_model(X_train_tensor)\n",
    "    \n",
    "    # Loss Calculation\n",
    "    loss = cox_loss(risk, train_times, train_events)\n",
    "    \n",
    "    # Backward Pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch_losses.append(loss.item())\n",
    "\n",
    "print(\"‚úÖ 'best_model' is now trained and ready for the Ensemble step.\")\n",
    "\n",
    "# Optional: Plot the training curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epoch_losses)\n",
    "plt.title(\"DeepSurv Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cox Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703aeb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 9: RANDOM SURVIVAL FOREST WITH GRID SEARCH\n",
    "# ---------------------------------------------------------\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# 1. Use the Polynomial Features (X_final from previous steps)\n",
    "# We need to re-split because X_final has the interaction terms\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training RSF on {X_train_poly.shape[1]} features (including interactions)...\")\n",
    "\n",
    "# 2. Define Hyperparameter Grid (The \"Model Selection\" Step)\n",
    "param_grid = {\n",
    "    'n_estimators': [500],\n",
    "    'min_samples_split': [10, 15, 20],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "# 3. Run Grid Search (5-Fold CV)\n",
    "rsf = RandomSurvivalForest(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rsf,\n",
    "    param_grid=param_grid,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Grid Search for Random Forest (this may take 2-3 mins)...\")\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "\n",
    "# 4. Get Best Model\n",
    "best_rsf = grid_search.best_estimator_\n",
    "print(f\"\\n‚úÖ Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"‚úÖ Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 5. Final Evaluation on Test Set\n",
    "rsf_score = best_rsf.score(X_test_poly, y_test)\n",
    "print(f\"\\nüéØ Optimized RSF C-Index: {rsf_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 10: ENSEMBLE MODEL (DeepSurv + RSF)\n",
    "# ---------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from scipy.stats import zscore\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîß Building Ensemble Model...\")\n",
    "\n",
    "# 1. Re-Create Train/Test Split using the POLYNOMIAL Data (X_final)\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Convert to PyTorch Tensors (The \"Poly\" Tensors)\n",
    "X_train_tensor_poly = torch.tensor(X_train_poly.values.astype(np.float32), dtype=torch.float32)\n",
    "X_test_tensor_poly = torch.tensor(X_test_poly.values.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "# Normalize (Critical for Neural Networks)\n",
    "mean = X_train_tensor_poly.mean(dim=0)\n",
    "std = X_train_tensor_poly.std(dim=0) + 1e-5\n",
    "X_train_tensor_poly = (X_train_tensor_poly - mean) / std\n",
    "X_test_tensor_poly = (X_test_tensor_poly - mean) / std\n",
    "\n",
    "# 3. Re-Initialize Model with Correct Input Dimension\n",
    "input_dim = X_train_tensor_poly.shape[1]\n",
    "print(f\"Model Input Dimension set to: {input_dim}\")\n",
    "\n",
    "best_model = DynamicDeepSurv(input_dim=input_dim, layers=[32, 16], dropout=0.1)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "\n",
    "# 4. Re-Train on the Polynomial Data\n",
    "print(\"Re-Training Final Champion Neural Network...\")\n",
    "for epoch in range(200): \n",
    "    best_model.train()\n",
    "    risk = best_model(X_train_tensor_poly)\n",
    "    loss = cox_loss(risk, train_times, train_events)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 5. Generate Final Predictions\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    risk_deep = best_model(X_test_tensor_poly).numpy().flatten()\n",
    "\n",
    "# 6. Get RSF Predictions\n",
    "risk_rf = best_rsf.predict(X_test_poly)\n",
    "\n",
    "# 7. ENSEMBLE: Normalize and Average\n",
    "risk_deep_norm = zscore(risk_deep)\n",
    "risk_rf_norm = zscore(risk_rf)\n",
    "\n",
    "# Weighted Average (0.6 RF + 0.4 Deep)\n",
    "ensemble_risk = (0.6 * risk_rf_norm) + (0.4 * risk_deep_norm)\n",
    "\n",
    "# 8. Calculate Final Scores\n",
    "c_deep = concordance_index_censored(test_events.squeeze().bool().numpy(), test_times.squeeze().numpy(), risk_deep)[0]\n",
    "c_rf = concordance_index_censored(test_events.squeeze().bool().numpy(), test_times.squeeze().numpy(), risk_rf)[0]\n",
    "c_ens = concordance_index_censored(test_events.squeeze().bool().numpy(), test_times.squeeze().numpy(), ensemble_risk)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"üèÜ FINAL RESULTS TABLE\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Random Forest: {c_rf:.4f}\")\n",
    "print(f\"DeepSurv (Poly): {c_deep:.4f}\")\n",
    "print(f\"ENSEMBLE: {c_ens:.4f}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4321149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 11: PREDICTION VISUALIZATION & MODEL COMPARISON\n",
    "# ---------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# ============================\n",
    "# 1. Head View of Final Predictions\n",
    "# ============================\n",
    "\n",
    "# Extract Event and Time from structured array y_test\n",
    "event_status = np.array([x[0] for x in y_test])  # Event (True=Deceased, False=Alive)\n",
    "survival_months = np.array([x[1] for x in y_test])  # Time in months\n",
    "\n",
    "# Create a comprehensive results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'RSF_Risk': risk_rf,\n",
    "    'DeepSurv_Risk': risk_deep,\n",
    "    'Ensemble_Risk': ensemble_risk,\n",
    "    'RSF_Risk_Norm': risk_rf_norm,\n",
    "    'DeepSurv_Risk_Norm': risk_deep_norm,\n",
    "    'Survival_Months': survival_months,\n",
    "    'Event_Status': event_status,\n",
    "    'Status_Label': ['Deceased' if s else 'Alive' for s in event_status]\n",
    "})\n",
    "\n",
    "# Sort by ensemble risk (highest risk first)\n",
    "results_df_sorted = results_df.sort_values('Ensemble_Risk', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä TOP 15 HIGHEST RISK PATIENTS (Ensemble Prediction)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df_sorted[['RSF_Risk_Norm', 'DeepSurv_Risk_Norm', 'Ensemble_Risk', \n",
    "                          'Survival_Months', 'Status_Label']].head(15).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TOP 15 LOWEST RISK PATIENTS (Ensemble Prediction)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df_sorted[['RSF_Risk_Norm', 'DeepSurv_Risk_Norm', 'Ensemble_Risk', \n",
    "                          'Survival_Months', 'Status_Label']].tail(15).to_string())\n",
    "\n",
    "# ============================\n",
    "# 2. Model Comparison Visualization\n",
    "# ============================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('üî¨ Three-Model Prediction Comparison: RSF vs DeepSurv vs Ensemble', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# --- Plot 1: Overlapping Distribution of Risk Scores ---\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(risk_rf_norm, bins=40, alpha=0.5, label=f'RSF (C={c_rf:.4f})', color='blue', edgecolor='black')\n",
    "ax1.hist(risk_deep_norm, bins=40, alpha=0.5, label=f'DeepSurv (C={c_deep:.4f})', color='green', edgecolor='black')\n",
    "ax1.hist(ensemble_risk, bins=40, alpha=0.5, label=f'Ensemble (C={c_ens:.4f})', color='red', edgecolor='black')\n",
    "ax1.set_xlabel('Normalized Risk Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Risk Scores (All Models)')\n",
    "ax1.legend()\n",
    "\n",
    "# --- Plot 2: Scatter - RSF vs DeepSurv Predictions ---\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['red' if s else 'green' for s in event_status]\n",
    "ax2.scatter(risk_rf_norm, risk_deep_norm, c=colors, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "ax2.plot([-3, 3], [-3, 3], 'k--', alpha=0.5, label='Perfect Agreement')\n",
    "ax2.set_xlabel('RSF Risk Score (Normalized)')\n",
    "ax2.set_ylabel('DeepSurv Risk Score (Normalized)')\n",
    "ax2.set_title('RSF vs DeepSurv Agreement')\n",
    "legend_elements = [Patch(facecolor='red', label='Deceased'),\n",
    "                   Patch(facecolor='green', label='Alive')]\n",
    "ax2.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "# --- Plot 3: Bar Chart - C-Index Comparison ---\n",
    "ax3 = axes[0, 2]\n",
    "models = ['Random\\nSurvival Forest', 'DeepSurv\\n(Neural Net)', 'Ensemble\\n(60% RF + 40% Deep)']\n",
    "c_indices = [c_rf, c_deep, c_ens]\n",
    "colors_bar = ['#3498db', '#27ae60', '#e74c3c']\n",
    "bars = ax3.bar(models, c_indices, color=colors_bar, edgecolor='black', linewidth=1.5)\n",
    "ax3.set_ylabel('C-Index Score')\n",
    "ax3.set_title('Model Performance Comparison')\n",
    "ax3.set_ylim(0.5, max(c_indices) + 0.05)\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random Baseline')\n",
    "for bar, c_val in zip(bars, c_indices):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{c_val:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "ax3.legend()\n",
    "\n",
    "# --- Plot 4: Patient Ranking Comparison ---\n",
    "ax4 = axes[1, 0]\n",
    "patient_indices = np.arange(len(risk_rf_norm))\n",
    "sorted_idx_ens = np.argsort(ensemble_risk)\n",
    "ax4.scatter(patient_indices, risk_rf_norm[sorted_idx_ens], alpha=0.6, s=15, label='RSF', color='blue')\n",
    "ax4.scatter(patient_indices, risk_deep_norm[sorted_idx_ens], alpha=0.6, s=15, label='DeepSurv', color='green')\n",
    "ax4.scatter(patient_indices, ensemble_risk[sorted_idx_ens], alpha=0.8, s=15, label='Ensemble', color='red')\n",
    "ax4.set_xlabel('Patient Rank (by Ensemble Risk)')\n",
    "ax4.set_ylabel('Normalized Risk Score')\n",
    "ax4.set_title('Risk Predictions Across All Patients')\n",
    "ax4.legend()\n",
    "\n",
    "# --- Plot 5: Box Plot by Event Status ---\n",
    "ax5 = axes[1, 1]\n",
    "alive_mask = event_status == False\n",
    "deceased_mask = event_status == True\n",
    "\n",
    "box_data = [\n",
    "    risk_rf_norm[alive_mask], risk_rf_norm[deceased_mask],\n",
    "    risk_deep_norm[alive_mask], risk_deep_norm[deceased_mask],\n",
    "    ensemble_risk[alive_mask], ensemble_risk[deceased_mask]\n",
    "]\n",
    "box_labels = ['RSF\\nAlive', 'RSF\\nDeceased', 'Deep\\nAlive', 'Deep\\nDeceased', 'Ens\\nAlive', 'Ens\\nDeceased']\n",
    "bp = ax5.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "colors_box = ['lightgreen', 'lightcoral'] * 3\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "ax5.set_ylabel('Normalized Risk Score')\n",
    "ax5.set_title('Risk Distribution by Survival Status')\n",
    "ax5.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# --- Plot 6: Risk Group Analysis ---\n",
    "ax6 = axes[1, 2]\n",
    "risk_quantiles = pd.qcut(ensemble_risk, q=4, labels=['Low Risk', 'Medium-Low', 'Medium-High', 'High Risk'])\n",
    "risk_analysis = pd.DataFrame({\n",
    "    'Risk_Group': risk_quantiles,\n",
    "    'RSF_Risk': risk_rf_norm,\n",
    "    'DeepSurv_Risk': risk_deep_norm,\n",
    "    'Ensemble_Risk': ensemble_risk,\n",
    "    'Survival_Months': survival_months,\n",
    "    'Event': event_status\n",
    "})\n",
    "\n",
    "group_stats = risk_analysis.groupby('Risk_Group').agg({\n",
    "    'RSF_Risk': 'mean',\n",
    "    'DeepSurv_Risk': 'mean', \n",
    "    'Ensemble_Risk': 'mean',\n",
    "    'Event': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "x = np.arange(len(group_stats))\n",
    "width = 0.25\n",
    "ax6.bar(x - width, group_stats['RSF_Risk'], width, label='RSF', color='#3498db')\n",
    "ax6.bar(x, group_stats['DeepSurv_Risk'], width, label='DeepSurv', color='#27ae60')\n",
    "ax6.bar(x + width, group_stats['Ensemble_Risk'], width, label='Ensemble', color='#e74c3c')\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(group_stats.index, rotation=15)\n",
    "ax6.set_ylabel('Mean Risk Score')\n",
    "ax6.set_title('Mean Risk by Quartile Group')\n",
    "ax6.legend()\n",
    "\n",
    "for i, (idx, row) in enumerate(group_stats.iterrows()):\n",
    "    ax6.text(i, max(row['RSF_Risk'], row['DeepSurv_Risk'], row['Ensemble_Risk']) + 0.1, \n",
    "             f'Mort: {row[\"Event\"]*100:.0f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 3. Summary Statistics Table\n",
    "# ============================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<25} {'C-Index':<12} {'Mean Risk':<15} {'Std Risk':<15}\")\n",
    "print(\"-\"*67)\n",
    "print(f\"{'Random Survival Forest':<25} {c_rf:<12.4f} {risk_rf_norm.mean():<15.4f} {risk_rf_norm.std():<15.4f}\")\n",
    "print(f\"{'DeepSurv Neural Network':<25} {c_deep:<12.4f} {risk_deep_norm.mean():<15.4f} {risk_deep_norm.std():<15.4f}\")\n",
    "print(f\"{'Ensemble (60% RF + 40% DL)':<25} {c_ens:<12.4f} {ensemble_risk.mean():<15.4f} {ensemble_risk.std():<15.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Model comparison visualization saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62656221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 12: FEATURE IMPORTANCE (Permutation)\n",
    "# ---------------------------------------------------------\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚è≥ Calculating Feature Importance (this takes ~1-2 minutes)...\")\n",
    "\n",
    "# 1. Calculate Importance on Test Set\n",
    "result = permutation_importance(\n",
    "    best_rsf, X_test_poly, y_test, n_repeats=5, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2. Sort and Select Top 15 Features\n",
    "sorted_idx = result.importances_mean.argsort()[-15:]\n",
    "top_features = X_test_poly.columns[sorted_idx]\n",
    "top_scores = result.importances[sorted_idx]\n",
    "\n",
    "# 3. Enhanced Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "box = plt.boxplot(\n",
    "    top_scores.T,\n",
    "    vert=False,\n",
    "    labels=top_features,\n",
    "    patch_artist=True,\n",
    "    widths=0.7\n",
    ")\n",
    "\n",
    "# Apply Professional Coloring\n",
    "for patch in box['boxes']:\n",
    "    patch.set_facecolor('#2ecc71')\n",
    "    patch.set_alpha(0.6)\n",
    "    patch.set_edgecolor('black')\n",
    "\n",
    "for median in box['medians']:\n",
    "    median.set_color('black')\n",
    "    median.set_linewidth(1.5)\n",
    "\n",
    "# Add Guidelines\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "plt.axvline(x=0, color='red', linestyle='-', linewidth=1, label=\"No Impact\")\n",
    "\n",
    "# Labels\n",
    "plt.title(\"Top 15 Drivers of Survival Prediction (Feature Importance)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Decrease in Model Accuracy (C-Index) when Feature is Removed\", fontsize=12)\n",
    "plt.ylabel(\"Clinical Features & Interactions\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Print Summary for Report\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üèÜ TOP 5 PREDICTIVE FEATURES\")\n",
    "print(\"=\"*40)\n",
    "for i in range(1, 6):\n",
    "    idx = sorted_idx[-i]\n",
    "    print(f\"{i}. {X_test_poly.columns[idx]} (Impact: {result.importances_mean[idx]:.4f})\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CELL 13: SHAP ANALYSIS (Model Explainability)\n",
    "# ---------------------------------------------------------\n",
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üöÄ Initializing SHAP Analysis for the Hybrid Ensemble...\")\n",
    "\n",
    "# 1. Define the Ensemble Wrapper Function\n",
    "def ensemble_predict_wrapper(X_numpy):\n",
    "    if isinstance(X_numpy, np.ndarray):\n",
    "        X_df = pd.DataFrame(X_numpy, columns=X_test_poly.columns)\n",
    "    else:\n",
    "        X_df = X_numpy\n",
    "\n",
    "    # Random Forest Prediction\n",
    "    risk_rf = best_rsf.predict(X_df)\n",
    "\n",
    "    # DeepSurv Prediction\n",
    "    X_tensor = torch.tensor(X_df.values.astype(np.float32), dtype=torch.float32)\n",
    "    train_mean = X_train_tensor_poly.mean(dim=0)\n",
    "    train_std = X_train_tensor_poly.std(dim=0) + 1e-5\n",
    "    X_tensor_norm = (X_tensor - train_mean) / train_std\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        risk_deep = best_model(X_tensor_norm).numpy().flatten()\n",
    "\n",
    "    return (0.6 * risk_rf) + (0.4 * risk_deep)\n",
    "\n",
    "# 2. Prepare Background Data (Summary)\n",
    "print(\"‚öôÔ∏è  Summarizing Training Data for Baseline...\")\n",
    "X_train_summary = shap.kmeans(X_train_poly, 20)\n",
    "\n",
    "# 3. Initialize Kernel Explainer on the ENSEMBLE\n",
    "explainer = shap.KernelExplainer(ensemble_predict_wrapper, X_train_summary)\n",
    "\n",
    "# 4. Run on the Full Test Set\n",
    "print(f\"‚ö° Running SHAP on all {X_test_poly.shape[0]} test patients...\")\n",
    "print(\"   (This may take 2-5 mins)\")\n",
    "shap_values = explainer.shap_values(X_test_poly)\n",
    "\n",
    "# 5. Plot the Results\n",
    "print(\"üé® Generating Plots...\")\n",
    "\n",
    "# Summary Plot (Global Feature Importance)\n",
    "plt.figure()\n",
    "plt.title(\"What drives risk in the Hybrid Ensemble?\")\n",
    "shap.summary_plot(shap_values, X_test_poly, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top Interaction Plot\n",
    "top_feature = X_test_poly.columns[0]\n",
    "shap.dependence_plot(top_feature, shap_values, X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63069f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
